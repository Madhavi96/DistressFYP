{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistressFYP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJfskhT4OYCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import joblib\n",
        "import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "np.random.seed(15)\n",
        "import pandas as pd\n",
        "import scipy.io.wavfile as wav\n",
        "from numpy.lib import stride_tricks\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import backend as K\n",
        "K.common.set_image_dim_ordering('th')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNLbF6HdOaBe",
        "colab_type": "text"
      },
      "source": [
        "# Silent removal and Speaker diarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWL-h03g9-rI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from SilentRemoval_SpeakerDiarization import remove_silence\n",
        "\n",
        "# directory containing raw wav files\n",
        "dir_name = 'dataset'\n",
        "\n",
        "# directory where a participant folder will be created containing their\n",
        "# segmented wav file\n",
        "out_dir = 'processed_dataset'\n",
        "\n",
        "# iterate through wav files in dir_name and create a segmented wav_file\n",
        "for file in os.listdir(dir_name):\n",
        "  if file.endswith('.wav'):\n",
        "    filename = os.path.join(dir_name, file)\n",
        "    remove_silence(filename, out_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8a6vhC9OMjk",
        "colab_type": "text"
      },
      "source": [
        "# Augment depressed audio and generate new samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-WcBLUI_UDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv('train_split_Depression_AVEC2017.csv')\n",
        "df_validate = pd.read_csv('dev_split_Depression_AVEC2017.csv')\n",
        "df_dev = pd.concat([df_train, df_validate], axis=0)\n",
        "\n",
        "for i in os.listdir('processed_dataset'):\n",
        "  id = int(i[1:])\n",
        "  if id in df_dev['Participant_ID'].values:\n",
        "    if df_dev.loc[df_dev['Participant_ID'] == id]['PHQ8_Binary'].item() == 1:\n",
        "      y, sr = librosa.load('processed_dataset/'+i+'/'+i+'_no_silence.wav')  \n",
        "      y_changed = librosa.effects.time_stretch(y, rate=1.07)\n",
        "      librosa.output.write_wav('augmented_dataset/'+i+'_no_silence.wav' ,y_changed, sr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cZrij64zgkk",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction from train dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEB5pTEuzp-b",
        "colab_type": "text"
      },
      "source": [
        "From original preprocessed wav files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvNOlg3nzUd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FeatureExtraction import stft_matrix\n",
        "from FeatureExtraction import get_random_samples\n",
        "\n",
        "depressed_data = []\n",
        "normal_data = []\n",
        "\n",
        "base = 'processed_dataset/'\n",
        "\n",
        "for i in os.listdir(base):\n",
        "  if int(i[1:]) in set(df_dev['Participant_ID'].values):\n",
        "    try:\n",
        "      data = get_random_samples(stft_matrix(base+i+'/'+i+'_no_silence.wav'),46,125)\n",
        "      if (df_dev.loc[df_dev['Participant_ID'] == int(i[1:])]['PHQ8_Binary'].item()==0):\n",
        "        normal_data = normal_data + data\n",
        "      else:\n",
        "        depressed_data = depressed_data + data\n",
        "    except:\n",
        "      print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X41rXO-D0HR4",
        "colab_type": "text"
      },
      "source": [
        "From augmented depressed wav files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx6ztg4J0CWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base = 'augmented_dataset/'\n",
        "for i in os.listdir(base):\n",
        "  x=int(i[1:4])\n",
        "  if x in set(df_dev['Participant_ID'].values):\n",
        "    try:\n",
        "      data = get_random_samples(stft_matrix(base+i),46,125)\n",
        "      depressed_data = depressed_data + data\n",
        "    except:\n",
        "      print(i)\n",
        "\n",
        "print(np.array(depressed_data).shape)\n",
        "print(np.array(normal_data).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM_UwbYbOweO",
        "colab_type": "text"
      },
      "source": [
        "Seprate train, validation datasets for CNN and GSOM models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hixub-cl0xgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(depressed_data)\n",
        "random.shuffle(depressed_data)\n",
        "\n",
        "cnn_train_data = depressed_data[:1702]+ normal_data[:1802]\n",
        "cnn_validation_data = depressed_data[1702:3202]+ normal_data[1802:3402]\n",
        "gsom_validation_data = depressed_data[3202:]+ normal_data[3402:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg-pqao81MB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_train_labels = []\n",
        "cnn_validation_labels = []\n",
        "gsom_validation_labels = []\n",
        "\n",
        "for i in range(1702):\n",
        "  cnn_train_labels.append(1)\n",
        "for i in range(1802):\n",
        "  cnn_train_labels.append(0)\n",
        "\n",
        "for i in range(1500):\n",
        "  cnn_validation_labels.append(1)\n",
        "for i in range(1600):\n",
        "  cnn_validation_labels.append(0)\n",
        "\n",
        "for i in range(202):\n",
        "  gsom_validation_labels.append(1)\n",
        "for i in range(232):\n",
        "  gsom_validation_labels.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28vn3lxk1WzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joblib.dump(np.array(cnn_train_data),'cnn_train_data.joblib')\n",
        "joblib.dump(np.array(cnn_validation_data),'cnn_validation_data.joblib')\n",
        "joblib.dump(np.array(gsom_validation_data),'gsom_validation_data.joblib')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441B9UY6lXH5",
        "colab_type": "text"
      },
      "source": [
        "# CNN model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iif3hnt0PBTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FeaturePreprocess import prep_train_test\n",
        "from FeaturePreprocess import keras_img_prep\n",
        "from CNNmodel import cnn\n",
        "from Evaluation import cnn_model_performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzYBAbHhPGpA",
        "colab_type": "text"
      },
      "source": [
        "CNN model configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duKeHoiUPJwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "epochs = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v53ZQ71kPqqr",
        "colab_type": "text"
      },
      "source": [
        "preprocess train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIb8bR-mPu6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalalize data and prep for Keras\n",
        "X_train, X_test, y_train, y_test = prep_train_test(cnn_train_data, np.array(cnn_train_labels),cnn_validation_data, np.array(cnn_validation_labels),nb_classes=nb_classes)\n",
        "\n",
        "# 513x125x1 for spectrogram with crop size of 125 pixels\n",
        "img_rows, img_cols, img_depth = X_train.shape[1], X_train.shape[2], 1\n",
        "\n",
        "# reshape image input for Keras\n",
        "# used Theano dim_ordering (th), (# chans, # images, # rows, # cols)\n",
        "X_train, X_test, input_shape = keras_img_prep(X_train, X_test, img_depth,img_rows, img_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZJi53NPy9-",
        "colab_type": "text"
      },
      "source": [
        "Fit the cnn model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFzMiUgfP633",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, history = cnn(X_train, y_train, X_test, y_test, batch_size,nb_classes, epochs, input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p9pCfJyP9cE",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRqlU2Z9QAZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_pred, y_test_pred, y_train_pred_proba, y_test_pred_proba, conf_matrix = cnn_model_performance(model, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# custom evaluation metrics\n",
        "print('Calculating additional test metrics...')\n",
        "accuracy = float(conf_matrix[0][0] + conf_matrix[1][1]) / np.sum(conf_matrix)\n",
        "precision = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[0][1])\n",
        "recall = float(conf_matrix[0][0]) / (conf_matrix[0][0] + conf_matrix[1][0])\n",
        "f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "print(\"Precision: {}\".format(precision))\n",
        "print(\"Recall: {}\".format(recall))\n",
        "print(\"F1-Score: {}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJs5Yh_e1hR",
        "colab_type": "text"
      },
      "source": [
        "# GSOM model creation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiGIyTCBQ9ZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from FeaturePreprocess import prep_full_test\n",
        "\n",
        "sys.path.append('GSOM')\n",
        "\n",
        "import data_parser as Parser\n",
        "from util import utilities as Utils\n",
        "from util import display as Display_Utils\n",
        "from params import params as Params\n",
        "from core4 import core_controller as Core"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf25AVO2R_Kk",
        "colab_type": "text"
      },
      "source": [
        "CNN validation data output to train GSOM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BSOObofSFX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_validation_data = joblib.load('cnn_validation_data.joblib')\n",
        "\n",
        "gsom_train_data = prep_full_test(cnn_validation_data)\n",
        "\n",
        "# 513x125x1 for spectrogram with crop size of 125 pixels\n",
        "img_rows, img_cols, img_depth = gsom_train_data.shape[1], gsom_train_data.shape[2], 1\n",
        "\n",
        "# reshape image input for Keras\n",
        "# used Theano dim_ordering (th), (# chans, # images, # rows, # cols)\n",
        "gsom_train_data, input_shape = keras_img_prep(gsom_train_data, img_depth, img_rows, img_cols)\n",
        "\n",
        "cnnPredictions=model.predict(gsom_train_data)\n",
        "cnnPredictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "091K2c86Syzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def array_to_comma_separated(array):\n",
        "    return ','.join([str(i) for i in array]) \n",
        "\n",
        "f=open(\"GSOM/data/cnn_5100_input_file_to_gsom\",\"w\")       \n",
        "for i in range(len(cnnPredictions)):\n",
        "  f.write(str(i)+\",\"+array_to_comma_separated(cnnPredictions[i])+\",\"+str(cnn_validation_labels[i])+\"\\n\")  \n",
        "\n",
        "f=open(\"GSOM/data/cnn_5100_input_file_to_gsom\")\n",
        "len(f.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gek8uPMzfHm9",
        "colab_type": "text"
      },
      "source": [
        "GSOM configs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_mrYHmmE0X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SF = 0.7\n",
        "forget_threshold = 80  # To include forgetting, threshold should be < learning iterations.\n",
        "temporal_contexts = 1  # If stationary data - keep this at 1\n",
        "learning_itr = 100\n",
        "smoothing_irt = 50\n",
        "plot_for_itr = 4  # Unused parameter - just for visualization. Keep this as it is.\n",
        "\n",
        "# File Config\n",
        "data_filename = \"GSOM/data/cnn_5100_input_file_to_gsom\"\n",
        "output_save_location = 'GSOM/output/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhhdJsl6tBDw",
        "colab_type": "text"
      },
      "source": [
        "GSOM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRgHCCvks_i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from GSOMmodel import GSOM_model\n",
        "\n",
        "GSOM_model(SF,forget_threshold,temporal_contexts,learning_itr,smoothing_irt,plot_for_itr,data_filename,output_save_location,\"5100_for_0_7\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZSnRMrAgav8",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate CNN+GSOM Combined Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-I85ARikFyq",
        "colab_type": "text"
      },
      "source": [
        "Load CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZrZRurDifzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('cnn_5100.h5')\n",
        "\n",
        "model.pop()\n",
        "model.pop()\n",
        "model.pop()\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oFWiQtakJbZ",
        "colab_type": "text"
      },
      "source": [
        "Load validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh5gc2f5YOZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gsom_validation_data = joblib.load('gsom_validation_data.joblib')\n",
        "\n",
        "gsom_validation_data = prep_full_test(gsom_validation_data)\n",
        "\n",
        "# 513x125x1 for spectrogram with crop size of 125 pixels\n",
        "img_rows, img_cols, img_depth = gsom_validation_data.shape[1], gsom_validation_data.shape[2], 1\n",
        "\n",
        "# reshape image input for Keras\n",
        "# used Theano dim_ordering (th), (# chans, # images, # rows, # cols)\n",
        "gsom_validation_data, input_shape = keras_img_prep(gsom_validation_data, img_depth, img_rows, img_cols)\n",
        "\n",
        "cnnPredictions=model.predict(gsom_validation_data)\n",
        "cnnPredictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_UerfHVYVxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open(\"GSOM/data/cnn_5100_validate_file_to_gsom\",\"w\")\n",
        "for i in range(len(cnnPredictions)):\n",
        "  f.write(str(i)+\",\"+array_to_comma_separated(cnnPredictions[i])+\",\"+str(gsom_validation_labels[i])+\"\\n\") \n",
        "\n",
        "f=open(\"GSOM/data/cnn_5100_validate_file_to_gsom\")\n",
        "len(f.readlines()) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFUHKp1TX-3",
        "colab_type": "text"
      },
      "source": [
        "Load test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzcExy9Jh6o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_full_test = joblib.load('X_full_test_saved.joblib')\n",
        "cnnPredictionsForGsomTest = model.predict(X_full_test)\n",
        " \n",
        "f=open(\"GSOM/data/cnn_5100_test_file_to_gsom\",\"w\")\n",
        "for i in range(len(cnnPredictionsForGsomTest)):\n",
        "  f.write(str(i)+\",\"+array_to_comma_separated(cnnPredictionsForGsomTest[i])+\"\\n\")\n",
        "\n",
        "f=open(\"GSOM/data/cnn_5100_test_file_to_gsom\")\n",
        "print(len(f.readlines()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzUlzv1QXz70",
        "colab_type": "text"
      },
      "source": [
        "Node labeling algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjGXKu67iJqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import data_parser as Parser\n",
        "\n",
        "input_vector_database, labels, classes = Parser.InputParser.parse_input_train_data('GSOM/data/cnn_5100_input_file_to_gsom', None)\n",
        "input_vector_database_validate, validate_labels = Parser.InputParser.parse_input_test_data('GSOM/data/cnn_5100_validate_file_to_gsom', None)\n",
        "input_vector_database_test, test_labels = Parser.InputParser.parse_input_test_data('GSOM/data/cnn_5100_test_file_to_gsom', None)\n",
        "\n",
        "def get_labels_in_radius(gsom_nodemap,radius,x,y):  \n",
        "  label_list = get_winner_labels(gsom_nodemap,x+radius,y)+get_winner_labels(gsom_nodemap,x-radius,y)+get_winner_labels(gsom_nodemap,x,y+radius)+get_winner_labels(gsom_nodemap,x,y-radius)+get_winner_labels(gsom_nodemap,x+radius,y+radius)+get_winner_labels(gsom_nodemap,x+radius,y-radius)+get_winner_labels(gsom_nodemap,x-radius,y+radius)+get_winner_labels(gsom_nodemap,x-radius,y-radius)\n",
        "  return label_list\n",
        "\n",
        "def get_winner_labels(gsom_nodemap,n,m):\n",
        "  winner_key = Utils.Utilities.generate_index(n, m)\n",
        "  try:\n",
        "    mapped_input_labels=gsom_nodemap[winner_key].get_mapped_labels()\n",
        "    return [str(classes[lbl_id]) for lbl_id in mapped_input_labels]\n",
        "\n",
        "  except KeyError:\n",
        "    #if the new generated key does not exist in the original key map\n",
        "    return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jUS2_RZYLYK",
        "colab_type": "text"
      },
      "source": [
        "Evaluate gsom on validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYEoVyujXtji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions_from_gsom(gsom_nodemap,threshold):\n",
        "  test_predictions=[]\n",
        "\n",
        "  for test_input_id in validate_labels:\n",
        "    #print('test_input_id: ',test_input_id)\n",
        "    winner=Utils.Utilities.select_winner(gsom_nodemap, input_vector_database_validate[0][test_input_id], Params.DistanceFunction.EUCLIDEAN, -1)\n",
        "    label_list = get_winner_labels(gsom_nodemap,winner.x,winner.y)\n",
        "\n",
        "    radius=0\n",
        "    x = winner.x\n",
        "    y = winner.y\n",
        "\n",
        "    while(len(label_list)==0):      \n",
        "      radius=radius+1\n",
        "      label_list=label_list+get_labels_in_radius(gsom_nodemap,radius,x,y)\n",
        "      \n",
        "    maxCountElement = max(label_list,key=label_list.count)\n",
        "\n",
        "    a=label_list.count('1')\n",
        "    b=label_list.count('0')\n",
        "    p=a/(a+b)\n",
        "\n",
        "    if p>=threshold:\n",
        "      test_predictions.append(1)\n",
        "    else:\n",
        "      test_predictions.append(0)\n",
        "\n",
        "  print('test_predictions ',test_predictions)\n",
        "  print('num test_predictions: ',len(test_predictions))\n",
        "  return test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2W3rWwqYXca",
        "colab_type": "text"
      },
      "source": [
        "Evaluate combined model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7N3OYpVXlTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions_from_gsom_test(gsom_nodemap,threshold):\n",
        "  test_predictions=[]\n",
        "\n",
        "  for test_input_id in test_labels:\n",
        "    #print('test_input_id: ',test_input_id)\n",
        "    winner=Utils.Utilities.select_winner(gsom_nodemap, input_vector_database_test[0][test_input_id], Params.DistanceFunction.EUCLIDEAN, -1)\n",
        "    label_list = get_winner_labels(gsom_nodemap,winner.x,winner.y)\n",
        "\n",
        "    radius=0\n",
        "    x = winner.x\n",
        "    y = winner.y\n",
        "\n",
        "    while(len(label_list)==0):      \n",
        "      radius=radius+1\n",
        "      label_list=label_list+get_labels_in_radius(gsom_nodemap,radius,x,y)\n",
        "    \n",
        "    a=label_list.count('1')\n",
        "    b=label_list.count('0')\n",
        "    p=a/(a+b)\n",
        "\n",
        "    #maxCountElement = max(label_list,key=label_list.count)\n",
        "    if p>=threshold:\n",
        "      test_predictions.append(1)\n",
        "    else:\n",
        "      test_predictions.append(0)\n",
        "\n",
        "  print('test_predictions ',test_predictions)\n",
        "  print('num test_predictions: ',len(test_predictions))\n",
        "\n",
        "  bundled_test_predictions=[]\n",
        "  for i in range(0,37):\n",
        "    start_idx=46*i\n",
        "    end_idx=start_idx+46\n",
        "    bundled_test_predictions.append(int(np.round(np.average(test_predictions[start_idx:end_idx]))))\n",
        "    #print(bundled_test_predictions)\n",
        "  np.asarray(bundled_test_predictions)\n",
        "  print('bundled_test_predictions ',bundled_test_predictions)\n",
        "  print('num bundled_test_predictions:',len(bundled_test_predictions))\n",
        "\n",
        "  return bundled_test_predictions,test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXASg-b4YdYv",
        "colab_type": "text"
      },
      "source": [
        "Summary of final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOdbWgPOjPgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Evaluation import evaluate_accuracies, overall_evaluation_test, overall_evaluation_unbundle \n",
        "\n",
        "threshold = 0.35\n",
        "\n",
        "gsom_nodemap_5000_for_0_5 = joblib.load('GSOM/output/gsom_nodemap_5100_for_0_7.joblib')\n",
        "prediction_validate = get_predictions_from_gsom(gsom_nodemap_5000_for_0_5,threshold)\n",
        "evaluate_accuracies(prediction_validate)\n",
        "print(\"****************************************\")\n",
        "predictions,test_predictions = get_predictions_from_gsom_test(gsom_nodemap_5000_for_0_5,threshold)\n",
        "overall_evaluation_test(predictions)\n",
        "overall_evaluation_unbundle(test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}